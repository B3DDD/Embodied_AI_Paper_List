# Embodied_AI_Paper_List
Paper list for Embodied AI 

<p align="center">
<img src="./EmbodiedAI.jpg" width="250">
</p>


#### We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to **liuy856@mail.sysu.edu.cn** and **chen867820261@gmail.com**. Thanks for your cooperation!

**Aligning Cyber Space with Physical World: A Survey on Embodied AI**, 
  [[Yang Liu](https://yangliu9208.github.io)], Weixing Chen, Yongjie Bai, Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Zhida Li, Ganlong Zhao, Junyi Lin, Guanbin Li, Wen Gao, Liang Lin 
  [[PDF](https://arxiv.org/pdf/2407.06886)] 
<p align="center">
<img src="./survey.jpg" width="800">
</p>  

## :collision: Update Log 
* [2024.07.10] We release the first version of the survey on Embodied AI [PDF](https://arxiv.org/pdf/2407.06886)!
* [2024.07.10] We release the first version of the paper list for Embodied AI. This page is continually updating!



## <a id="table-of-contents">üìö Table of Contents </a>

- [Books & Surveys](#books-surveys)
- [Embodied Simulators](#simulators)
- [Embodied Perception](#perception)
- [Embodied Interaction](#interaction)
- [Embodied Agent](#agent)
- [Sim-to-Real Adaptation](#sim-to-real)

## <a id="books-surveys"> Books & Surveys <a href="#table-of-contents">üîù</a> </a> 

* **Multimodal Large Models: The New Paradigm of Artificial General Intelligence**, Publishing House of Electronics Industry (PHE), 2024     
Yang Liu, Liang Lin     
[[Page](https://hcplab-sysu.github.io/Book-of-MLM/)]      

* **A Survey on Vision-Language-Action Models for Embodied AI**, arXiv:2405.14093, 2024   
Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King    
[[Paper](https://arxiv.org/pdf/2405.14093)]

* **Towards Generalist Robot Learning from Internet Video: A Survey**, arXiv:2404.19664, 2024   
McCarthy, Robert, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, and Zhibin Li.  
[[Paper](https://arxiv.org/pdf/2404.19664)]

* **Toward general-purpose robots via foundation models: A survey and meta-analysis**, arXiv:2312.08782, 2023   
Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim et al.  
[[Paper](https://arxiv.org/pdf/2312.08782)]    

* **A survey of embodied ai: From simulators to research tasks**, IEEE Transactions on Emerging Topics in Computational Intelligence, 2022    
Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan    
[[Paper](https://arxiv.org/pdf/2103.04918)]    

* **The development of embodied cognition: Six lessons from babies**, Artificial life, 2005    
Linda Smith, Michael Gasser    
[[Paper](https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf)]    

* **Embodied artificial intelligence: Trends and challenges**, Lecture notes in computer science, 2004    
Rolf Pfeifer, Fumiya Iida   
[[Paper](https://people.csail.mit.edu/iida/papers/PfeiferIidaEAIDags.pdf)]     

## <a id="simulators"> Embodied Simulators <a href="#table-of-contents">üîù</a> </a>
### General Simulator


* **Nvidia isaac sim: Robotics simulation and synthetic data**, NVIDIA, 2023

* **Design and use paradigms for gazebo, an open-source multi-robot simulator**, IROS, 2004
Koenig, Nathan, Andrew, Howard. 

* **Pybullet, a python module for physics simulation for games, robotics and machine learning**, 2016.
Coumans, Erwin, Yunfei, Bai. 

* **Webots: open-source robot simulator**
Cyberbotics

* **MuJoCo: A physics engine for model-based control**, IROS, 2012.
Todorov, Emanuel, Tom, Erez, Yuval, Tassa. 

* **Unity: A general platform for intelligent agents**, arXiv, 2020
Juliani, Arthur, Vincent-Pierre, Berges, Ervin, Teng, Andrew, Cohen, Jonathan, Harper, Chris, Elion, Chris, Goy, Yuan, Gao, Hunter, Henry, Marwan, Mattar, Danny, Lange. 

* **AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles**, Field and Service Robotics. 2017.
Shital Shah, , Debadeepta Dey, Chris Lovett, Ashish Kapoor. 

* **MORSE: the Modular Open Robots Simulator Engine**
ISAE-SUPAERO

* **V-REP: A versatile and scalable robot simulation framework**, IROS 2013.
Rohmer, Eric, Surya PN, Singh, Marc, Freese. 



### Real-Scene Based Simulators
* **ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation**, NeurIPS, 2021.
Gan, Chuang, J., Schwartz, Seth, Alter, Martin, Schrimpf, James, Traer, JulianDe, Freitas, Jonas, Kubilius, Abhishek, Bhandwaldar, Nick, Haber, Megumi, Sano, Kuno, Kim, Elias, Wang, Damian, Mrowca, Michael, Lingelbach, Aidan, Curtis, KevinT., Feigelis, DavidM., Bear, Dan, Gutfreund, DavidD., Cox, JamesJ., DiCarlo, JoshH., McDermott, JoshuaB., Tenenbaum, Daniel, Yamins. 
[[page]()]

* **iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes**, IROS, 2021.
Shen, Bokui, Fei, Xia, Chengshu, Li, Roberto, Mart√≠n-Mart√≠n, Linxi, Fan, Guanzhi, Wang, Claudia, P√©rez-D‚ÄôArpino, Shyamal, Buch, Sanjana, Srivastava, Lyne, Tchapmi, Micael, Tchapmi, Kent, Vainio, Josiah, Wong, Li, Fei-Fei, Silvio, Savarese. 

* **SAPIEN: A SimulAted Part-Based Interactive ENvironment**, CVPR, 2020.
Xiang, Fanbo, Yuzhe, Qin, Kaichun, Mo, Yikuan, Xia, Hao, Zhu, Fangchen, Liu, Minghua, Liu, Hanxiao, Jiang, Yifu, Yuan, He, Wang, Li, Yi, Angel X., Chang, Leonidas J., Guibas, Hao, Su. 

* **Habitat: A Platform for Embodied AI Research**, ICCV, 2019.
Savva, Manolis, Abhishek, Kadian, Oleksandr, Maksymets, Yili, Zhao, Erik, Wƒ≥mans, Bhavana, Jain, Julian, Straub, Jia, Liu, Vladlen, Koltun, Jitendra, Malik, Devi, Parikh, Dhruv, Batra. 

* **VirtualHome: Simulating Household Activities Via Programs**, CVPR, 2018.
Puig, Xavier, Kevin, Ra, Marko, Boben, Jiaman, Li, Tingwu, Wang, Sanja, Fidler, Antonio, Torralba. 

* **Matterport3D: Learning from RGB-D Data in Indoor Environments**, 3DV, 2017.
Chang, Angel, Angela, Dai, Thomas, Funkhouser, Maciej, Halber, Matthias, Niebner, Manolis, Savva, Shuran, Song, Andy, Zeng, Yinda, Zhang. 

* **AI2-THOR: An Interactive 3D Environment for Visual AI**. arXiv, 2017.
Kolve, Eric, Roozbeh, Mottaghi, Daniel, Gordon, Yuke, Zhu, Abhinav, Gupta, Ali, Farhadi. 

* **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation**, arXiv, 2023.
Wang, Yufei, Zhou, Xian, Feng, Chen, Tsun-Hsuan, Wang, Yian, Wang, Katerina, Fragkiadaki, Zackory, Erickson, David, Held, Chuang, Gan. 

* **Holodeck: Language Guided Generation of 3D Embodied AI Environments**, CVPR, 2024.
Yue Yang, , Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark. 

* **PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI**, CVPR, 2024
Yang, Yandan, Baoxiong, Jia, Peiyuan, Zhi, Siyuan, Huang. 

## <a id="perception">  Embodied Perception <a href="#table-of-contents">üîù</a> </a>
### Active Visual Exploration

* **MonoSLAM: Real-time single camera SLAM**, IEEE T-PAMI 29. 6(2007): 1052‚Äì1067.
Davison, Andrew J, Ian D, Reid, Nicholas D, Molton, Olivier, Stasse. 

* **A multi-state constraint Kalman filter for vision-aided inertial navigation**, IROS, 2007.
Mourikis, Anastasios I, Stergios I, Roumeliotis. 

* **Parallel tracking and mapping for small AR workspaces**, ISMAR, 2007.
Klein, Georg, David, Murray. 

* **ORB-SLAM: a versatile and accurate monocular SLAM system** IEEE T-RO 31. 5(2015): 1147‚Äì1163.
Mur-Artal, Raul, Jose Maria Martinez, Montiel, Juan D, Tardos. 

* **DTAM: Dense tracking and mapping in real-time**, ICCV, 2011.
Newcombe, Richard A, Steven J, Lovegrove, Andrew J, Davison. 

* **LSD-SLAM: Large-scale direct monocular SLAM**, ECCV, 2014.
Engel, Jakob, Thomas, Schops, Daniel, Cremers. 

* **Slam++: Simultaneous localisation and mapping at the level of objects**, CVPR, 2013.
Salas-Moreno, Renato F, Richard A, Newcombe, Hauke, Strasdat, Paul HJ, Kelly, Andrew J, Davison. 

* **Cubeslam: Monocular 3-d object slam**, IEEE T-RO 35. 4(2019): 925‚Äì938.
Yang, Shichao, Sebastian, Scherer. 

* **Hierarchical topic model based object association for semantic SLAM**, IEEE T-VCG 25. 11(2019): 3052‚Äì3062.
Zhang, Jianhua, Mengping, Gui, Qichao, Wang, Ruyu, Liu, Junzhe, Xu, Shengyong, Chen. 

* **Quadricslam: Dual quadrics from object detections as landmarks in object-oriented slam**, IEEE Robotics and Automation Letters 4. 1(2018): 1‚Äì8.
Nicholson, Lachlan, Michael, Milford, Niko, S√ºnderhauf. 

* **So-slam: Semantic object slam with scale proportional and symmetrical texture constraints**. IEEE Robotics and Automation Letters 7. 2(2022): 4008‚Äì4015.
Liao, Ziwei, Yutong, Hu, Jiadong, Zhang, Xianyu, Qi, Xiaoyu, Zhang, Wei, Wang. 

* **DS-SLAM: A semantic visual SLAM towards dynamic environments**, IROS, 2018.
Yu, Chao, Zuxin, Liu, Xin-Jun, Liu, Fugui, Xie, Yi, Yang, Qi, Wei, Qiao, Fei. 

* **DynaSLAM: Tracking, mapping, and inpainting in dynamic scenes**, IEEE Robotics and Automation Letters 3. 4(2018): 4076‚Äì4083.
Bescos, Berta, Jos√© M, Facil, Javier, Civera, Jos√©, Neira. 

* **SG-SLAM: A real-time RGB-D visual SLAM toward dynamic scenes with semantic and geometric information**, IEEE Transactions on Instrumentation and Measurement 72. (2022): 1‚Äì12.
Cheng, Shuhong, Changhe, Sun, Shƒ≥un, Zhang, Dianfan, Zhang. 

* **OVD-SLAM: An online visual SLAM for dynamic environments**, IEEE Sensors Journal, 2023.
He, Jiaming, Mingrui, Li, Yangyang, Wang, Hongyu, Wang. 

* **Gs-slam: Dense visual slam with 3d gaussian splatting**, CVPR, 2024.
Yan, Chi, Delin, Qu, Dan, Xu, Bin, Zhao, Zhigang, Wang, Dong, Wang, Xuelong, Li. 

* **Multi-view 3d object detection network for autonomous driving**, CVPR, 2017.
Chen, Xiaozhi, Huimin, Ma, Ji, Wan, Bo, Li, Tian, Xia. 

* **Pointpillars: Fast encoders for object detection from point clouds**, CVPR, 2019.
Lang, Alex H, Sourabh, Vora, Holger, Caesar, Lubing, Zhou, Jiong, Yang, Oscar, Beijbom. 

* **Multi-view convolutional neural networks for 3d shape recognition**, ICCV, 2015.
Su, Hang, Subhransu, Maji, Evangelos, Kalogerakis, Erik, Learned-Miller. 

* **Voxnet: A 3d convolutional neural network for real-time object recognition**, IROS, 2015.
Maturana, Daniel, Sebastian, Scherer. 

* **Semantic scene completion from a single depth image**, CVPR, 2017.
Song, Shuran, Fisher, Yu, Andy, Zeng, Angel X, Chang, Manolis, Savva, Thomas, Funkhouser. 

* **4d spatio-temporal convnets: Minkowski convolutional neural networks**, CVPR, 2019.
Choy, Christopher, JunYoung, Gwak, Silvio, Savarese. 

* **3d semantic segmentation with submanifold sparse convolutional networks**, CVPR, 2018.
Graham, Benjamin, Martin, Engelcke, Laurens, Van Der Maaten. 

* **Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai** CVPR, 2024.
Wang, Tai, Xiaohan, Mao, Chenming, Zhu, Runsen, Xu, Ruiyuan, Lyu, Peisen, Li, Xiao, Chen, Wenwei, Zhang, Kai, Chen, Tianfan, Xue, others. 

* **Pointnet: Deep learning on point sets for 3d classification and segmentation**, CVPR, 2017.
Qi, Charles R, Hao, Su, Kaichun, Mo, Leonidas J, Guibas. 

* **Pointnet++: Deep hierarchical feature learning on point sets in a metric space**, NeurIPS, 2017
Qi, Charles Ruizhongtai, Li, Yi, Hao, Su, Leonidas J, Guibas. 

Ma, Xu, Can, Qin, Haoxuan, You, Haoxi, Ran, Yun, Fu. 
* **Rethinking network design and local geometry in point cloud: A simple residual MLP framework**, arXiv, 2022.

Zhao, Hengshuang, Li, Jiang, Jiaya, Jia, Philip HS, Torr, Vladlen, Koltun. 
* **Point transformer**, ICCV, 2021.

* **Swin3d: A pretrained transformer backbone for 3d indoor scene understanding**, arXiv, 2023.
Yang, Yu-Qi, Yu-Xiao, Guo, Jian-Yu, Xiong, Yang, Liu, Hao, Pan, Peng-Shuai, Wang, Xin, Tong, Baining, Guo. 

* **Point transformer v2: Grouped vector attention and partition-based pooling** NeurIPS, 2022
Wu, Xiaoyang, Yixing, Lao, Li, Jiang, Xihui, Liu, Hengshuang, Zhao. 

* **Point Transformer V3: Simpler Faster Stronger**, CVPR, 2024.
Wu, Xiaoyang, Li, Jiang, Peng-Shuai, Wang, Zhijian, Liu, Xihui, Liu, Yu, Qiao, Wanli, Ouyang, Tong, He, Hengshuang, Zhao. 

* **PointMamba: A Simple State Space Model for Point Cloud Analysis**, arXiv, 2024.
Liang, Dingkang, Xin, Zhou, Xinyu, Wang, Xingkui, Zhu, Wei, Xu, Zhikang, Zou, Xiaoqing, Ye, Xiang, Bai. 

* **Point Could Mamba: Point Cloud Learning via State Space Model**, arXiv, 2024.
Zhang, Tao, Xiangtai, Li, Haobo, Yuan, Shunping, Ji, Shuicheng, Yan. 

* **Mamba3d: Enhancing local features for 3d point cloud analysis via state space model** arXiv, 2024.
Han, Xu, Yuan, Tang, Zhaoxuan, Wang, Xianzhi, Li. 

* **The curious robot: Learning visual representations via physical interactions**, ECCV, 2016.
Pinto, Lerrel, Dhiraj, Gandhi, Yuanfeng, Han, Yong-Lae, Park, Abhinav, Gupta. 

* **Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies**, ICRA, 2023.
Tatiya, Gyan, Jonathan, Francis, Jivko, Sinapov. 

* **Learning to look around: Intelligently exploring unseen environments for unknown tasks**, CVPR, 2018.
Jayaraman, Dinesh, Kristen, Grauman. 

* **Neu-nbv: Next best view planning using uncertainty estimation in image-based neural rendering**, IROS, 2023.
Jin, Liren, Xieyuanli, Chen, Julius, R√ºckin, Marija, Popovi\'c. 

* **Off-policy evaluation with online adaptation for robot exploration in challenging environments**, IEEE Robotics and Automation Letters, 2023.
Hu, Yafei, Junyi, Geng, Chen, Wang, John, Keller, Sebastian, Scherer. 

* **Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception**, CVPR, 2024.
Fan, Lei, Mingfu, Liang, Yunxuan, Li, Gang, Hua, Ying, Wu. 


### 3D Visual Grounding
* **ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language**, ECCV, 2020
Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias 
[[page]](https://arxiv.org/pdf/1912.08830)

* **ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes**, ECCV, 2020
Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas
[[page]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf)

* **Text-guided graph neural networks for referring 3D instance segmentation**, AAAI, 2021
Huang, Pin-Hao and Lee, Han-Hung and Chen, Hwann-Tzong and Liu, Tyng-Luh
[[page]](https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060)

* **InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring**, ICCV, 2021
Yuan, Zhihao and Yan, Xu and Liao, Yinghong and Zhang, Ruimao and Wang, Sheng and Li, Zhen and Cui, Shuguang
[[page]](https://arxiv.org/pdf/2103.01128)

* **Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud**, CVPR, 2021
Feng, Mingtao and Li, Zhen and Li, Qi and Zhang, Liang and Zhang, XiangDong and Zhu, Guangming and Zhang, Hui and Wang, Yaonan and Mian, Ajmal
[[page]](https://arxiv.org/pdf/2103.16381)

* **SAT: 2D Semantics Assisted Training for 3D Visual Grounding**, CVPR, 2021
Yang, Zhengyuan and Zhang, Songyang and Wang, Liwei and Luo, Jiebo
[[page]](https://arxiv.org/pdf/2105.11450)

* **LanguageRefer: Spatiallanguage model for 3D visual grounding**, CVPR, 2021
Roh, Junha and Desingh, Karthik and Farhadi, Ali and Fox, Dieter
[[page]](https://arxiv.org/pdf/2107.03438)

* **3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds**, ICCV, 2021
Zhao, Lichen and Cai, Daigang and Sheng, Lu and Xu, Dong
[[page]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf)

* **TransRefer3D: Entity-and-relation aware transformer for fine-grained 3D visual grounding**, CVPR, 2021
He, Dailan and Zhao, Yusheng and Luo, Junyu and Hui, Tianrui and Huang, Shaofei and Zhang, Aixi and Liu, Si
[[page]](https://arxiv.org/pdf/2108.02388)

* **Multi-view transformer for 3D visual grounding**, CVPR, 2022
Huang, Shijia and Chen, Yilun and Jia, Jiaya and Wang, Liwei
[[page]](https://arxiv.org/pdf/2204.02174)

* **Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**, CVPR, 2022
Bakr, Eslam and Alsaedy, Yasmeen and Elhoseiny, Mohamed
[[page]](https://arxiv.org/pdf/2211.14241)

* **LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent**, arXix, 2023
Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce
[[page]](https://arxiv.org/pdf/2309.12311)

* **Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**, arXix, 2023
Yuan, Zhihao and Ren, Jinke and Feng, Chun-Mei and Zhao, Hengshuang and Cui, Shuguang and Li, Zhen
[[page]](https://arxiv.org/pdf/2311.15383)

* **3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**, CVPR, 2022
Luo, Junyu and Fu, Jiahui and Kong, Xianghao and Gao, Chen and Ren, Haibing and Shen, Hao and Xia, Huaxia and Liu, Si
[[page]](https://arxiv.org/pdf/2204.06272)

* **Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds**, ECCV, 2022
Jain, Ayush and Gkanatsios, Nikolaos and Mediratta, Ishita and Fragkiadaki, Katerina
[[page]](https://arxiv.org/pdf/2112.08879)

* **EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**, CVPR, 2023
Wu, Yanmin and Cheng, Xinhua and Zhang, Renrui and Cheng, Zesen and Zhang, Jian
[[page]](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf)


### Visual Language Navigation


* **Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments**, CVPR, 2018.
Anderson, Peter, Qi, Wu, Damien, Teney, Jake, Bruce, Mark, Johnson, Niko, Sunderhauf, Ian, Reid, Stephen, Gould, Anton, Hengel. 

* **Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation**, ACL, 2019.
Jain, Vihan, Gabriel, Magalhaes, Alexander, Ku, Ashish, Vaswani, Eugene, Ie, Jason, Baldridge. 

* **Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments**, ECCV, 2020,
Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan.

* **TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments**, CVPR, 2019.
Chen, Howard, Alane, Suhr, Dipendra, Misra, Noah, Snavely, Yoav, Artzi. 

* **REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments**, CVPR, 2020.
Qi, Yuankai, Qi, Wu, Peter, Anderson, Xin, Wang, William Yang, Wang, Chunhua, Shen, Anton, Hengel. 

* **SOON: Scenario Oriented Object Navigation with Graph-based Exploration**, CVPR, 2021.
Zhu, Fengda, Xiwen, Liang, Yi, Zhu, Qizhi, Yu, Xiaojun, Chang, Xiaodan, Liang. 

* **ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks**, CVPR, 2020.
Shridhar, Mohit, Jesse, Thomason, Daniel, Gordon, Yonatan, Bisk, Winson, Han, Roozbeh, Mottaghi, Luke, Zettlemoyer, Dieter, Fox. 

* **HomeRobot: Open-Vocabulary Mobile Manipulation**, arXiv, 2023.
Yenamandra, Sriram, Arun, Ramachandran, Karmesh, Yadav, Austin, Wang, Mukul, Khanna, Theophile, Gervet, Tsung-Yen, Yang, Vidhi, Jain, AlexanderWilliam, Clegg, John, Turner, Zsolt, Kira, Manolis, Savva, Angel, Chang, DevendraSingh, Chaplot, Dhruv, Batra, Roozbeh, Mottaghi, Yonatan, Bisk, Chris, Paxton. 

* **Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation**, Conference on Robot Learning. 2023.
Li, Chengshu, Ruohan, Zhang, Josiah, Wong, Cem, Gokmen, Sanjana, Srivastava, Roberto, Mart\in-Mart\'\in, Chen, Wang, Gabrael, Levine, Michael, Lingelbach, Jiankai, Sun, others. 

* **Vision-and-dialog navigation**, Conference on Robot Learning. 2020.
Thomason, Jesse, Michael, Murray, Maya, Cakmak, Luke, Zettlemoyer. 

* **DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following**, arXiv, 2022.
Gao, Xiaofeng, Qiaozi, Gao, Ran, Gong, Kaixiang, Lin, Govind, Thattai, GauravS., Sukhatme. 

* **Language and visual entity relationship graph for agent navigation**, NeurIPS, 2020.
Hong, Yicong, Cristian, Rodriguez, Yuankai, Qi, Qi, Wu, Stephen, Gould. 

* **Language-Guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning**, IEEE T-CSVT 31. (2020): 3469-3481.
Weixia Zhang, , Chao Ma, Qi Wu, Xiaokang Yang. 

* **Vision-Language Navigation Policy Learning and Adaptation**, IEEE T-PAMI 43. 12(2021): 4205-4216.
Wang, Xin, Qiuyuan, Huang, Asli, Celikyilmaz, Jianfeng, Gao, Dinghan, Shen, Yuan-Fang, Wang, William Yang, Wang, Lei, Zhang. 

* **FILM: Following Instructions in Language with Modular Methods**, ICLR, 2022.
So Yeon Min, , Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov. 

* **LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**, Conference on Robot Learning. 2022.
Dhruv Shah, , Blazej Osinski, Brian Ichter, Sergey Levine. 

* **HOP: History-and-Order Aware Pretraining for Vision-and-Language Navigation**, CVPR, 2022.
Qiao, Yanyuan, Yuankai, Qi, Yicong, Hong, Zheng, Yu, Peng, Wang, Qi, Wu. 

* **Towards Learning a Generalist Model for Embodied Navigation**, CVPR, 2024.
Duo Zheng, , Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang. 

* **Fast-Slow Test-time Adaptation for Online Vision-and-Language Navigation** ICML, 2024.
Junyu Gao, , Xuan Yao, Changsheng Xu. 

* **Discuss before moving: Visual language navigation via multi-expert discussions**, ICRA, 2024.
Long, Yuxing, Xiaoqi, Li, Wenzhe, Cai, Hao, Dong. 

* **Vision-and-Language Navigation via Causal Learning**, CVPR, 2024.
Liuyi Wang, Qijun Chen. 

* **Volumetric Environment Representation for Vision-Language Navigation**, CVPR, 2024.
Rui Liu, Yi Yang. 

* **NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation**, ArXiv, 2024.
Jiazhao Zhang, , Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He. 

* **Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation**, ECCV, 2018.
Xin Eric Wang, , Wenhan Xiong, Hongmin Wang, William Yang Wang. 

* **Neighbor-view enhanced model for vision and language navigation**, MM, 2021.
An, Dong, Yuankai, Qi, Yan, Huang, Qi, Wu, Liang, Wang, Tieniu, Tan. 

* **Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation**, CVPR, 2022.
Hong, Yicong, Zun, Wang, Qi, Wu, Stephen, Gould. 

* **March in Chat: Interactive Prompting for Remote Embodied Referring Expression**, ICCV, 2023.
Qiao, Yanyuan, Yuankai, Qi, Zheng, Yu, Jing, Liu, Qi, Wu. 

* **Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation**, CVPR 2024.
Wang, Zihan, Xiangyang, Li, Jiahao, Yang, Yeqi, Liu, Junjie, Hu, Ming, Jiang, Shuqiang, Jiang. 

* **ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments**, IEEE T-PAMI, 2024.
An, Dong, Hanqing, Wang, Wenguan, Wang, Zun, Wang, Yan, Huang, Keji, He, Liang, Wang. 

* **Multi-level compositional reasoning for interactive instruction following**, AAAI, 2023.
Bhambri, Suvaansh, Byeonghwi, Kim, Jonghyun, Choi. 

* **Vision and Language Navigation in the Real World via Online Visual Language Mapping**, ArXiv, 2023.
Chengguang Xu, , Hieu T. Nguyen, Christopher Amato, Lawson L.S. Wong. 


## <a id="interaction"> Embodied Interaction <a href="#table-of-contents">üîù</a> </a> 
## <a id="agent"> Embodied Agent <a href="#table-of-contents">üîù</a> </a> 

## <a id="sim-to-real"> Sim-to-Real Adaptation <a href="#table-of-contents">üîù</a> </a> 

## :newspaper: Citation 
If you think this survey is helpful, please feel free to leave a star ‚≠êÔ∏è and cite our paper:

```bibtex
@article{Liu2024EAISurvey,
        title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI}, 
        author={Yang Liu, Weixing Chen, Yongjie Bai, Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Zhida Li, Ganlong Zhao, Junyi Lin, Guanbin Li, Wen Gao and Liang Lin},
        year={2024},
        journal={arXiv preprint arXiv:2407.06886},
}
```

